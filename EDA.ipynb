{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFBW5hFbj8EY"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/datasets/shahriarkabir/procurement-kpi-analysis-dataset\n",
        "\n",
        "\n",
        "# ============================\n",
        "# ðŸ“¦ Procurement KPI Dataset EDA\n",
        "# ============================\n",
        "\n",
        "# 1. Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 2. Load the data\n",
        "df = pd.read_csv('Procurement_KPI.csv')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "df.head()\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - We check if the data loads correctly.\n",
        "# - We see initial rows and size.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 3. Basic Info and Data Types\n",
        "df.info()\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - We identify data types (numerical, categorical, datetime).\n",
        "# - Important for later cleaning and modeling decisions.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 4. Check for Missing Values\n",
        "missing_values = df.isnull().sum().sort_values(ascending=False)\n",
        "print(missing_values)\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Missing values can bias the model.\n",
        "# - Strategy: If missing <5%, impute; if >30%, consider dropping.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 5. Check for Duplicates\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "# Drop duplicates if any\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Duplicate data can cause models to memorize and overfit.\n",
        "# - Safe to drop unless specified otherwise.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 6. Univariate Analysis\n",
        "# a) Categorical Columns (Example: 'Category')\n",
        "if 'Category' in df.columns:\n",
        "    df['Category'].value_counts().plot(kind='bar', title='Category Distribution')\n",
        "    plt.show()\n",
        "\n",
        "# b) Numerical Columns\n",
        "df.describe()\n",
        "\n",
        "# Plot histograms for numerical features\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[num_cols].hist(figsize=(15, 10), bins=30)\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - See distribution of numerical and categorical features.\n",
        "# - Detect skewness, rare categories, extreme values.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 7. Bivariate Analysis (Feature vs Target)\n",
        "\n",
        "# Example: Cost Savings vs Contract Value\n",
        "if 'Cost Savings' in df.columns and 'Contract Value' in df.columns:\n",
        "    sns.scatterplot(x='Contract Value', y='Cost Savings', data=df)\n",
        "    plt.title('Cost Savings vs Contract Value')\n",
        "    plt.show()\n",
        "\n",
        "# Grouped barplot example for Region (if available)\n",
        "if 'Region' in df.columns:\n",
        "    region_means = df.groupby('Region')['Cost Savings'].mean().sort_values()\n",
        "    region_means.plot(kind='bar', title='Average Cost Savings by Region')\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Understand relationships with target variable.\n",
        "# - Detect important features visually.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 8. Multivariate Analysis - Correlation\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Find highly correlated features.\n",
        "# - Remove multicollinearity if correlation > 0.8 between two input features.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 9. Feature Engineering\n",
        "\n",
        "# Example: Create a new feature Savings Percentage\n",
        "if 'Cost Savings' in df.columns and 'Contract Value' in df.columns:\n",
        "    df['Savings Percentage'] = df['Cost Savings'] / df['Contract Value']\n",
        "\n",
        "# Example: Group rare suppliers/categories if needed\n",
        "if 'Supplier Name' in df.columns:\n",
        "    supplier_counts = df['Supplier Name'].value_counts()\n",
        "    rare_suppliers = supplier_counts[supplier_counts < 5].index\n",
        "    df['Supplier Name'] = df['Supplier Name'].replace(rare_suppliers, 'Other')\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Sometimes new features are better predictors than raw ones.\n",
        "# - Handling rare labels avoids data fragmentation.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 10. Outlier Detection and Treatment\n",
        "\n",
        "# Boxplot example\n",
        "if 'Cost Savings' in df.columns:\n",
        "    sns.boxplot(x=df['Cost Savings'])\n",
        "    plt.title('Boxplot of Cost Savings')\n",
        "    plt.show()\n",
        "\n",
        "# IQR method to remove extreme outliers\n",
        "Q1 = df['Cost Savings'].quantile(0.25)\n",
        "Q3 = df['Cost Savings'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Optional: Filter out extreme values\n",
        "df = df[(df['Cost Savings'] >= lower_bound) & (df['Cost Savings'] <= upper_bound)]\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Extreme outliers affect regression models heavily.\n",
        "# - Removing/capping them improves model robustness.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 11. Final Data Cleaning\n",
        "\n",
        "# a) Encode categorical variables\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# b) Scale numerical variables (Optional for tree models; Required for KNN, SVM, etc.)\n",
        "scaler = StandardScaler()\n",
        "scaled_cols = ['Contract Value', 'PO Cycle Time']\n",
        "\n",
        "for col in scaled_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = scaler.fit_transform(df[[col]])\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Encoding is needed to convert categorical to numerical.\n",
        "# - Scaling required if distance matters (e.g., KNN, SVM).\n",
        "# -----------------------------------------\n",
        "\n",
        "# 12. Model Selection Recommendation\n",
        "\n",
        "# If simple relationships (linear) âž” Linear Regression\n",
        "# If complex, non-linear relationships âž” Random Forest or XGBoost\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Linear models are simple and interpretable.\n",
        "# - Tree-based models handle messy, non-linear real-world data better.\n",
        "# - XGBoost/LightGBM are optimized for structured data competitions.\n",
        "# -----------------------------------------\n",
        "\n",
        "print(\"âœ… EDA Completed. Ready for Modeling!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/datasets/zahidmughal2343/global-cancer-patients-2015-2024\n",
        "\n",
        "\n",
        "# ============================\n",
        "# ðŸ“¦ Global Cancer Patients Dataset EDA\n",
        "# ============================\n",
        "\n",
        "# 1. Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 2. Load the data\n",
        "df = pd.read_csv('Global_Cancer_Patients.csv')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "df.head()\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Check if the data loads correctly.\n",
        "# - Understand initial size and get a feeling for the columns.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 3. Basic Info and Data Types\n",
        "df.info()\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Identify numerical, categorical, datetime columns.\n",
        "# - Important for choosing correct cleaning methods.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 4. Check for Missing Values\n",
        "missing_values = df.isnull().sum().sort_values(ascending=False)\n",
        "print(missing_values)\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Missing values can cause errors and bias.\n",
        "# - Strategy: Impute if <5% missing; drop or engineer if >30% missing.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 5. Check for Duplicates\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "# Drop duplicates if any\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Duplicated patient records can distort model learning.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 6. Univariate Analysis\n",
        "# a) Categorical Features\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "\n",
        "for col in cat_cols:\n",
        "    print(df[col].value_counts())\n",
        "    df[col].value_counts().plot(kind='bar', title=f'{col} Distribution')\n",
        "    plt.show()\n",
        "\n",
        "# b) Numerical Features\n",
        "df.describe()\n",
        "\n",
        "# Histograms\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[num_cols].hist(figsize=(15, 10), bins=30)\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Analyze the spread, outliers, and distribution of individual features.\n",
        "# - Important to detect imbalances and skewness.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 7. Bivariate Analysis (Feature vs Target)\n",
        "\n",
        "# Assuming target variable could be \"Survival Status\" (if exists) or another outcome.\n",
        "# Let's check survival rate across Age groups or Gender.\n",
        "\n",
        "if 'Survival Status' in df.columns and 'Age' in df.columns:\n",
        "    sns.boxplot(x='Survival Status', y='Age', data=df)\n",
        "    plt.title('Age distribution by Survival Status')\n",
        "    plt.show()\n",
        "\n",
        "if 'Survival Status' in df.columns and 'Gender' in df.columns:\n",
        "    sns.countplot(x='Gender', hue='Survival Status', data=df)\n",
        "    plt.title('Gender vs Survival Status')\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - See if age/gender affects survival rates.\n",
        "# - Find potential predictors.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 8. Multivariate Analysis - Correlation Matrix\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Identify multicollinearity (high correlation between input features).\n",
        "# - Important to avoid redundant information.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 9. Feature Engineering\n",
        "\n",
        "# Example ideas:\n",
        "# - Create Age bins (e.g., Child, Adult, Senior).\n",
        "# - Combine rare cancer types into 'Other' group.\n",
        "# - Calculate Treatment Delay (if treatment dates exist).\n",
        "\n",
        "if 'Age' in df.columns:\n",
        "    df['Age_Group'] = pd.cut(df['Age'], bins=[0, 18, 45, 65, 100], labels=['Child', 'Adult', 'Middle-Aged', 'Senior'])\n",
        "\n",
        "# Group rare categories\n",
        "for col in cat_cols:\n",
        "    rare_labels = df[col].value_counts()[df[col].value_counts() < 5].index\n",
        "    df[col] = df[col].replace(rare_labels, 'Other')\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Engineering meaningful groups can improve model performance.\n",
        "# - Avoid rare category fragmentation.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 10. Outlier Detection and Treatment\n",
        "\n",
        "# Example: Check outliers in Age\n",
        "if 'Age' in df.columns:\n",
        "    sns.boxplot(x=df['Age'])\n",
        "    plt.title('Boxplot of Age')\n",
        "    plt.show()\n",
        "\n",
        "# Remove extreme age values if needed\n",
        "Q1 = df['Age'].quantile(0.25)\n",
        "Q3 = df['Age'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "df = df[(df['Age'] >= lower_bound) & (df['Age'] <= upper_bound)]\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Remove unrealistic ages (negative ages, 150+ years old patients, etc.).\n",
        "# -----------------------------------------\n",
        "\n",
        "# 11. Final Data Cleaning\n",
        "\n",
        "# a) Encode categorical variables\n",
        "df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
        "\n",
        "# b) Scale numerical variables (Optional for Tree models; Required for KNN, SVM)\n",
        "scale_cols = df.select_dtypes(include=[np.number]).columns\n",
        "scaler = StandardScaler()\n",
        "df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Machine learning algorithms need clean numerical inputs.\n",
        "# - Scaling ensures features are comparable.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 12. Model Selection Recommendation\n",
        "\n",
        "# If the target is survival (binary classification):\n",
        "# - Simple data âž” Logistic Regression\n",
        "# - Complex patterns âž” Random Forest, XGBoost\n",
        "\n",
        "# If regression (e.g., predict Survival time in months):\n",
        "# - Simple âž” Linear Regression\n",
        "# - Complex âž” Random Forest Regressor, Gradient Boosting\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Choose model depending on problem type (classification or regression).\n",
        "# - Start simple, then move to ensemble methods if performance is low.\n",
        "# -----------------------------------------\n",
        "\n",
        "print(\"âœ… EDA Completed. Ready for Modeling!\")\n"
      ],
      "metadata": {
        "id": "UPOtH_tokPyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ðŸ“¦ Machine Learning Modeling Template\n",
        "# ============================\n",
        "\n",
        "# 1. Import Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# 2. Separate Features and Target\n",
        "\n",
        "# Define your target variable (replace 'TARGET_COLUMN' with your actual target)\n",
        "target = 'TARGET_COLUMN'\n",
        "\n",
        "X = df.drop(target, axis=1)\n",
        "y = df[target]\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Features (X) are all columns except target.\n",
        "# - Target (y) is what we want to predict.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 3. Train-Test Split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() <= 10 else None\n",
        ")\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - 80/20 split is a standard starting point.\n",
        "# - Use stratify for classification to maintain class balance.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 4. Model Selection Based on Problem Type\n",
        "\n",
        "# Check if itâ€™s classification or regression\n",
        "if y.nunique() <= 10:\n",
        "    problem_type = 'classification'\n",
        "else:\n",
        "    problem_type = 'regression'\n",
        "\n",
        "print(f\"Detected Problem Type: {problem_type}\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - If target has few unique values, it's likely classification.\n",
        "# - Otherwise, regression.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 5. Model Training\n",
        "\n",
        "if problem_type == 'classification':\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluation Metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, average='weighted')\n",
        "    rec = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Optional: ROC AUC if binary classification\n",
        "    if y.nunique() == 2:\n",
        "        y_proba = model.predict_proba(X_test)[:, 1]\n",
        "        roc = roc_auc_score(y_test, y_proba)\n",
        "        print(f\"ROC-AUC Score: {roc:.4f}\")\n",
        "\n",
        "else:\n",
        "    model = RandomForestRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluation Metrics\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"R2 Score: {r2:.4f}\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Use different metrics for classification (Accuracy, Precision, Recall, F1, ROC-AUC) vs regression (MAE, RMSE, R2).\n",
        "# - Random Forests are robust baseline models: they handle non-linearity and feature importance well.\n",
        "# -----------------------------------------\n",
        "\n",
        "# 6. Feature Importance (Optional)\n",
        "\n",
        "if hasattr(model, 'feature_importances_'):\n",
        "    importances = model.feature_importances_\n",
        "    feature_names = X.columns\n",
        "    feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "    # Plot Feature Importance\n",
        "    feat_imp.plot(kind='bar', figsize=(12,6), title='Feature Importances')\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------------------\n",
        "# ðŸ§  Reasoning:\n",
        "# - Understanding which features matter most helps explain model decisions.\n",
        "# - Especially important in interviews and production deployments.\n",
        "# -----------------------------------------\n",
        "\n",
        "# âœ… Modeling phase completed!\n"
      ],
      "metadata": {
        "id": "MdeFZwVgkV-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“„ One-Page EDA + Modeling Cheat Sheet\n",
        "\n",
        "# 1. Understand the Problem\n",
        "- What is the business goal?\n",
        "- Identify Target Variable.\n",
        "- Is it Classification (categories) or Regression (continuous numbers)?\n",
        "ðŸ§  Reasoning: Solve the right problem.\n",
        "\n",
        "# 2. Load the Data\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.read_csv('file.csv')\n",
        "df.head(), df.shape\n",
        "```\n",
        "ðŸ§  Reasoning: Confirm data loads and understand initial size.\n",
        "\n",
        "# 3. Data Types Check\n",
        "```python\n",
        "df.info()\n",
        "```\n",
        "ðŸ§  Reasoning: Know which columns are numerical, categorical, or datetime.\n",
        "\n",
        "# 4. Missing Values\n",
        "```python\n",
        "df.isnull().sum()\n",
        "```\n",
        "- <5% missing â†’ Impute (mean/median/mode)\n",
        "- >30% missing â†’ Drop or Engineer.\n",
        "ðŸ§  Reasoning: Handle missingness early to avoid bias.\n",
        "\n",
        "# 5. Duplicates\n",
        "```python\n",
        "df.duplicated().sum()\n",
        "df.drop_duplicates(inplace=True)\n",
        "```\n",
        "ðŸ§  Reasoning: Prevent model from overfitting repeated samples.\n",
        "\n",
        "# 6. Univariate Analysis\n",
        "- Numerical: `.describe()`, Histograms\n",
        "- Categorical: `.value_counts()`, Bar Plots\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "df.select_dtypes(include=[int, float]).hist(figsize=(15, 10), bins=30)\n",
        "plt.show()\n",
        "```\n",
        "ðŸ§  Reasoning: Understand feature distributions and spot outliers.\n",
        "\n",
        "# 7. Bivariate Analysis (Feature vs Target)\n",
        "- Numeric Target âž” Scatter plots, Correlations\n",
        "- Categorical Target âž” Boxplots, GroupBy Mean\n",
        "```python\n",
        "import seaborn as sns\n",
        "sns.scatterplot(x='Feature', y='Target', data=df)\n",
        "```\n",
        "ðŸ§  Reasoning: Find strong predictors.\n",
        "\n",
        "# 8. Multivariate Analysis\n",
        "```python\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "```\n",
        "ðŸ§  Reasoning: Detect multicollinearity (r > 0.8).\n",
        "\n",
        "# 9. Feature Engineering\n",
        "- Create ratios, bins, new features.\n",
        "- Group rare labels into \"Other\".\n",
        "```python\n",
        "df['Age_Group'] = pd.cut(df['Age'], bins=[0, 18, 45, 65, 100], labels=['Child', 'Adult', 'Middle-Aged', 'Senior'])\n",
        "```\n",
        "ðŸ§  Reasoning: Strong features improve models.\n",
        "\n",
        "# 10. Outlier Handling\n",
        "- Boxplots\n",
        "- IQR Method\n",
        "```python\n",
        "Q1 = df['col'].quantile(0.25)\n",
        "Q3 = df['col'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "df = df[(df['col'] >= Q1-1.5*IQR) & (df['col'] <= Q3+1.5*IQR)]\n",
        "```\n",
        "ðŸ§  Reasoning: Remove extreme values that skew models.\n",
        "\n",
        "# 11. Final Data Prep\n",
        "- Encode categorical variables\n",
        "- Scale numerical variables if needed (for KNN, SVM)\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "scaler = StandardScaler()\n",
        "num_cols = df.select_dtypes(include=[float, int]).columns\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "```\n",
        "ðŸ§  Reasoning: Models require clean numeric input.\n",
        "\n",
        "# 12. Train-Test Split\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df.drop('TARGET', axis=1)\n",
        "y = df['TARGET']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y if y.nunique()<=10 else None, random_state=42)\n",
        "```\n",
        "ðŸ§  Reasoning: Holdout data ensures unbiased evaluation.\n",
        "\n",
        "# 13. Choose Model\n",
        "| Problem Type | Suggested Model |\n",
        "|:-------------|:----------------|\n",
        "| Classification | Random Forest, XGBoost, Logistic Regression |\n",
        "| Regression | Random Forest Regressor, XGBoost Regressor, Linear Regression |\n",
        "\n",
        "ðŸ§  Reasoning: Tree models handle messy real-world data, linear for simple data.\n",
        "\n",
        "# 14. Model Training & Evaluation\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Detect Problem Type\n",
        "problem_type = 'classification' if y.nunique()<=10 else 'regression'\n",
        "\n",
        "# Train Model\n",
        "if problem_type == 'classification':\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "    print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "    print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
        "    if y.nunique() == 2:\n",
        "        print(\"ROC-AUC:\", roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))\n",
        "else:\n",
        "    model = RandomForestRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "    print(\"R2:\", r2_score(y_test, y_pred))\n",
        "```\n",
        "ðŸ§  Reasoning: Use metrics suitable for problem type â€” accuracy, F1 for classification; MAE, RMSE, R2 for regression.\n",
        "\n",
        "# 15. Feature Importance (optional)\n",
        "```python\n",
        "importances = model.feature_importances_\n",
        "feature_names = X.columns\n",
        "feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "feat_imp.plot(kind='bar', figsize=(12,6), title='Feature Importances')\n",
        "plt.show()\n",
        "```\n",
        "ðŸ§  Reasoning: Identify key drivers behind predictions.\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ“‹ Visual Workflow\n",
        "```\n",
        "Understand â†’ Load â†’ Clean â†’ Explore â†’ Engineer â†’ Prepare â†’ Model â†’ Evaluate\n",
        "```\n",
        "\n",
        "# ðŸ›¡ï¸ Interview Speaking Tip\n",
        "\"When explaining steps, always mention *why* you did something â€” not just *what* you did. Showing decision-making ability is more important than just technical execution.\"\n",
        "\n",
        "âœ…\n"
      ],
      "metadata": {
        "id": "lVNTolLtkiM8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}